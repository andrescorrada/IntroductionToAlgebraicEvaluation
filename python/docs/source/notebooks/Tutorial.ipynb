{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ccc343-0de7-430e-9c2c-9a77de944618",
   "metadata": {},
   "source": [
    "# Tutorial - evaluate three binary classifiers on a test from the UCI Adult dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83591f8-6965-4ff6-b7c3-b5525f99e8b7",
   "metadata": {},
   "source": [
    "This tutorial is going to walk through a single evaluation that was carried out on three binary classifiers trained and tested on the UCI Adult dataset. Two algebraic evaluators will be used. The first is the one you may be familiar with - using majority voting to impute the missing answer key to the unlabeled test. The second is hardly known to the ML/AI community - the exact, algebraic solution for error-independent ensembles and the basis of Data Engine's 2010 patent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1d0884-32ac-4b82-a842-1691c0435df0",
   "metadata": {},
   "source": [
    "![table](../../../img/uciAdultEvalPrevalenceGauges.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c51489-8ba9-49e5-b58e-6daa2b3bf6fa",
   "metadata": {},
   "source": [
    "The gauges above show that, for this particular test, the error-independent algebraic evaluator (AE) gets better estimates than the one obtained using majority voting (assuming the crowd is always right)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ac0e01-b02b-421e-9b3f-0bc65dc14c65",
   "metadata": {},
   "source": [
    "When you look at the estimated label accuracies for the three members of the ensemble in this test, the advantages of algebraic evaluation over majority voting become apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3ef2c-1eca-4bfe-87ad-7a585be063bd",
   "metadata": {},
   "source": [
    "![table](../../../img/uciAdultEvalPiaGauges.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5daab2-f7d5-4262-8abc-60c2fb1263d6",
   "metadata": {},
   "source": [
    "The majority voting evaluation is terrible. It incorrectly asserts that all three classifiers are worse than average on the \"a\" label, whereas, in reality, the problematic classifier here is the second one.\n",
    "\n",
    "This problematic evaluation by majority voting can be traced back to the assumption that underlies using the crowd to impute the missing answer key to any test - the crowd is always right. This not true in general - the classifiers could all be better than random guessers and they would **still** make wrong decisions as a group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732744df-d76c-4cff-9f48-adc305ea1ccf",
   "metadata": {},
   "source": [
    "## Walking through estimating the label prevalences in the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf1de6dd-fffc-49a4-8283-9810484cacda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import ntqr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d329c7-1b5b-47c5-b143-b9b02cb2a7c8",
   "metadata": {},
   "source": [
    "There is a data sketch of a test using the UCI Adult dataset that contains the by-label voting counts of three binary classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4222eebc-c683-41aa-a73c-a27d4db166b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_counts = ntqr.uciadult_label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41cfb173-2a82-4f20-b969-13bcbb32c0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': {('a', 'a', 'a'): 715,\n",
      "       ('a', 'a', 'b'): 161,\n",
      "       ('a', 'b', 'a'): 2406,\n",
      "       ('a', 'b', 'b'): 455,\n",
      "       ('b', 'a', 'a'): 290,\n",
      "       ('b', 'a', 'b'): 94,\n",
      "       ('b', 'b', 'a'): 1335,\n",
      "       ('b', 'b', 'b'): 231},\n",
      " 'b': {('a', 'a', 'a'): 271,\n",
      "       ('a', 'a', 'b'): 469,\n",
      "       ('a', 'b', 'a'): 3395,\n",
      "       ('a', 'b', 'b'): 7517,\n",
      "       ('b', 'a', 'a'): 272,\n",
      "       ('b', 'a', 'b'): 399,\n",
      "       ('b', 'b', 'a'): 6377,\n",
      "       ('b', 'b', 'b'): 12455}}\n"
     ]
    }
   ],
   "source": [
    "pprint(labeled_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69ffdd7-f3d8-45b8-a0a3-f292fdecbbc9",
   "metadata": {},
   "source": [
    "In an unsupervised setting, we do not know the true label for any item. All we get to see are the decisions the classifiers made. These are in effect, the sum of the above counts across the different labels. There is a class for operating on the labeled counts that can give us the projected counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d2ceb8b-cab0-4c2c-90b7-9fe3da9f7071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrioLabelVoteCounts(label_vote_counts={'a': {('a', 'a', 'a'): 715,\n",
      "                                             ('a', 'a', 'b'): 161,\n",
      "                                             ('a', 'b', 'a'): 2406,\n",
      "                                             ('a', 'b', 'b'): 455,\n",
      "                                             ('b', 'a', 'a'): 290,\n",
      "                                             ('b', 'a', 'b'): 94,\n",
      "                                             ('b', 'b', 'a'): 1335,\n",
      "                                             ('b', 'b', 'b'): 231},\n",
      "                                       'b': {('a', 'a', 'a'): 271,\n",
      "                                             ('a', 'a', 'b'): 469,\n",
      "                                             ('a', 'b', 'a'): 3395,\n",
      "                                             ('a', 'b', 'b'): 7517,\n",
      "                                             ('b', 'a', 'a'): 272,\n",
      "                                             ('b', 'a', 'b'): 399,\n",
      "                                             ('b', 'b', 'a'): 6377,\n",
      "                                             ('b', 'b', 'b'): 12455}})\n"
     ]
    }
   ],
   "source": [
    "trio_labeled_counts = ntqr.TrioLabelVoteCounts(labeled_counts)\n",
    "pprint(trio_labeled_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827a776-cbde-4679-b9eb-7c2762c673f8",
   "metadata": {},
   "source": [
    "The class TrioLabelVoteCounts can project the counts across labels for us and give us only the vote pattern counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8dcb119-0e5e-4217-8806-c42f6dc9a1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrioVoteCounts(vote_counts={('a', 'a', 'a'): 986,\n",
      "                            ('a', 'a', 'b'): 630,\n",
      "                            ('a', 'b', 'a'): 5801,\n",
      "                            ('a', 'b', 'b'): 7972,\n",
      "                            ('b', 'a', 'a'): 562,\n",
      "                            ('b', 'a', 'b'): 493,\n",
      "                            ('b', 'b', 'a'): 7712,\n",
      "                            ('b', 'b', 'b'): 12686})\n"
     ]
    }
   ],
   "source": [
    "trio_vote_counts = trio_labeled_counts.to_TrioVoteCounts()\n",
    "pprint(trio_vote_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fd84b-b102-40f7-b967-1fe15c323ba8",
   "metadata": {},
   "source": [
    "The challenge in unsupervised evaluation is to go from these counts back to the by-label counts. Let's see what happens when you use an algebraic evaluation that assumes that the classifiers were error independent on this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd7f951c-e0d8-490d-b35f-17012f826d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "algebraic_eval = ntqr.ErrorIndependentEvaluation(trio_vote_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc811dbf-46f5-45fb-b3e1-3d1d062b5024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11187722681*sqrt(3328641826009)/61316911076911789 + 1/2,\n",
      " 1/2 - 11187722681*sqrt(3328641826009)/61316911076911789]\n"
     ]
    }
   ],
   "source": [
    "ae_eval = algebraic_eval.evaluation_exact\n",
    "pprint(ae_eval[\"'a' prevalence solutions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a63d610-8760-4080-8d87-e9be50907d50",
   "metadata": {},
   "source": [
    "The two possible solutions for the 'a' label prevalence contain an unresolved square root. This means that the error independence assumption is wrong! The three classifiers in the test have non-zero error correlations. But let us see how close this irrational number is to the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44ad7515-a433-4d14-900b-d699d36deb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.832885560346949, 0.16711443965305095]\n"
     ]
    }
   ],
   "source": [
    "ae_evalf = algebraic_eval.evaluation_float\n",
    "pprint(ae_evalf[\"'a' prevalence solutions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c85e67b-7584-4a90-88f3-9d9760fc8afc",
   "metadata": {},
   "source": [
    "The correct evaluation can be computed with the labeled counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "004dd070-e66e-406f-81ee-5029387e123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_eval = ntqr.SupervisedEvaluation(trio_labeled_counts)\n",
    "seval_exact = supervised_eval.evaluation_exact\n",
    "seval_float = supervised_eval.evaluation_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d8678ba-9329-45c1-a0ac-c75e775f4bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.15436186960534173, 'b': 0.8456381303946583}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seval_float[\"prevalence\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec646576-5e6e-4af5-8855-aeede121d70a",
   "metadata": {},
   "source": [
    "So the error independent algebraic evaluation is 16.7% and the true 'a' label prevalence is 15.4%. Not bad. How does majority voting do on this test as an evaluator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61ec73ca-fdf5-4cb6-9bbe-d9bbbfc817e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_voting_eval = ntqr.MajorityVotingEvaluation(trio_vote_counts)\n",
    "mv_eval_exact = majority_voting_eval.evaluation_exact\n",
    "mv_eval_float = majority_voting_eval.evaluation_float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ca97ab-7077-40e7-aea9-94243afa9776",
   "metadata": {},
   "source": [
    "Majority voting does not warn you that the classifiers are actually error correlated since it can only produce rational estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd0fa17b-a634-49ad-a291-9f1bb7c23365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': Fraction(7979, 36842), 'b': Fraction(28863, 36842)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_eval_exact[\"prevalence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc872949-ac44-496b-a9d2-b3a6f9a7e0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.2165734759242169, 'b': 0.7834265240757831}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_eval_float[\"prevalence\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644276ef-3401-4136-812e-073d055d4755",
   "metadata": {},
   "source": [
    "To summarize - the true 'a' label prevalence is 15.4%, algebraic evaluation estimates 16.7%, and majority voting 21.7%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
