# A high-level, conceptual explanation of NTQR logic

When it comes to safety, there is strength in numbers. A group is safer than
an individual. One reason is that there are backups should an individual
in the group fail for any reason.

Another reason is that a group of decision makers allows us to use
"majority voting" to grade the members of a group. This well-known
"wisdom of the crowd" algorithm is based on the assumption that the crowd is
never wrong. Majority voting is commonly used for decision making, but it is
not the best algorithm to evaluate a crowd because sometimes the crowd is
wrong. By using these algorithms you can have your cake and eat it too - use
the crowd to grade itself, but not default to the majority vote to decide.

This is where NTQR algorithms excel in evaluation using unlabeled data. They
can detect when the "minority report" is correct and the crowd wrong. And,
to enhance safety, it can alert when an evaluation itself has gone wrong.

NTQR operates outside the box of conventional AI practice. This
mathematical and logical formalism is new and will become the foundation of
all future work on evaluation when you use unlabeled data. NTQR logic can
help you make your AI safer. It is the only logical choice when it comes to
AI safety.
